tools
=====
keras
tensorflow

Resources

https://towardsdatascience.com/building-a-convolutional-neural-network-cnn-in-keras-329fbbadc5f5
https://arxiv.org/abs/1707.05425
https://cs231n.github.io/convolutional-networks/
https://ml4a.github.io/ml4a/looking_inside_neural_nets/

maybe:
https://ml4a.github.io/ml4a/looking_inside_neural_nets/

failure case:
1999

== Methods ==
We attempt to predict how a given sensor with a Bayer filter responds to a given scene as if the width of the pixels were reduced by half - without performing a full simulation. Two linear transformations, inspired by demosaicking techniques, are used to predict the upsampled sensor response to establish baselines, representing a naive approach. In addition, we attempt to apply state-of-the-art neural network architectures, which have demonstrated success at a variety of image processing tasks, including image upsampling. We then compare the quality of the predicted upsampled sensor voltage data using several metrics.

=== Nearest Neighbor ===
A naive technique used for predicting upsampled sensor voltage data, the nearest neighbor algorithm simply takes the prior sensor response for a 4x4 "tile" and copies the values towards the right, beneath, and diagonally towards the bottom-right to double the size of the sensor response data. Nearest neighbor simply assumes that the additional pixels in the more dense sensor would have captured the same information regardless (which is obviously untrue). We use the predicted values of the nearest neighbor algorithm as a performance baseline.

=== Bilinear Interpolation ===
A slightly less naive technique, bilinear interpolation is inspired by the basic demosaicking algorithm of similar name. This algorithm averages the sensor voltage values of adjacent prior 4x4 tiles to produce new voltage values. Adjacency of a prior tile relative to a predicted tile is determined by whether the prior tiles is immediately present in the vertical, horizontal, or diagonal directions with a distance of 1. Tiles which are predicted at the edge of the sensor data will consist of the average of prior tiles which exist.

=== Convolutional Neural Networks ===
Inspired by state-of-the-art advances in image processing, we develop a convolution neural network architecture that is believed to represent being close to the extent to which a given sensor can be upsampled given no further information. Popularized in 2012 by Krizhevksy et. al [1], convolutional neural networks have been successfully applied to many other image processing tasks, including semantic segmentation, object detection, and image upsampling. In contrast to traditional neural networks, convolutional neural networks are especially suited towards image processing tasks due to lower number of weights to train (a naive 1-layer neural network for the task here would require the number of weights to train to be in the number of the hundreds of millions, while a single 3x3 kernel would only result in 9), as well as the validated assumption that the information required to predict the upsampled sensor data to be derived locally.

=== Dataset and Software Packages ===
* A ~2700 subset of the [http://images.cocodataset.org/zips/train2017.zip COCO 2017 Train images] was collected which included a variety of objects in various settings under different lighting scenarios, which would otherwise reflect the wide range of scenes that may be be captured by a camera. In addition, a few images of various facial profiles were derived from [https://github.com/isetbio/isetbio The Image System Engineering Toolbox for Biology (ISETBIO)]. For the neural network training, the sensor data was split into 2/3 for training and 1/3 for testing respectively.

* [https://keras.io Keras], [(https://www.tensorflow.org/ TensorFlow], and [https://scikit-learn.org Scikit-learn]  were used to construct, train, visualize, and test the convolutional neural network. [https://www.numpy.org/ NumPy] was used for data manipulation.

=== Generating Sensor Data ===

* These images were then processed as scenes using [https://github.com/ISET/isetcam ISETCAM] to produce a sensor response for both a 100 x 120 and a 202 x 242 sensor represented in voltages, which were used as the input and the target respectively. The two extra pixels in each dimension represent the fact that when the pixel size is halved, ISETCAM produced half a Bayer tile at the edges of the sensor response.

=== Postprocessing ===

[1] https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf
